## Lesson 1: Basic Concepts
  - [x] Machine learning in the real world
  - [x] Definition of machine learning
  - [x] Supervised/unsupervised learning
  - [x] Training/testing 
  - [x] Performance evaluation
  - [x] Empirical risk minimization
  - [x] Overfitting/underfitting
  - [ ] Additional Reading:
    - [ ] Bishop, Section 1.1
    - [x] Shalev-Schwartz, Chapter 2

## Lesson 2: Linear Predictors
  - [x] Least-squares regression
  - [x] Distance metrics and norms
  - [x] Ridge regression
  - [x] Gradient descent
  - [x] Lasso regression
  - [ ] Additional reading:
    - [ ] Bishop, Section 3.1
    - [ ] Hastie, Sections 3.1, 3.2, 3.4
 
## Lesson 3-4: Classification
  - [x] Logistic regression
  - [x] k-nearest neighbor classifier
  - [x] Precision/recall
  - [x] Receiver Operating Characteristics ROC 
  - [ ] Validation and hyperparameter tuning
  - [x] k-Fold Cross Validation
  - [ ] Additional Reading:
    - [ ] Bishop, Sections 4.1.1, 4.1.2, 4.1.3, 4.3.2, 4.3.4 and page 197

## Lesson 5-6-7: Probability theory
  - [x] Axioms, events, sigma-algebras 
  - [x] Conditional probability, Bayes theorem, and independence 
  - [x] Random variables, expectation, variance
  - [x] Probability density functions, cumulative distribution functions
  - [x] Distributions: Normal, Bernoulli, Dirichlet
  - [x] Estimators and bias
  - [x] Concentration inequalities (Markov, Chebyshev, Hoeffding) 
  - [ ] Additional Reading:
    - [ ] Bishop, Section 1.2
    - [ ] Mohri, Appendix C1-C5

## Lesson 8-9: Statistical learning theory
  - [x] Probably Approximately Correct (PAC) learning 
  - [x] Vapnik Chervonenkis dimension Vapnik Chervonenkis dimension
  - [x] The bias-variance dilemma 
  - [ ] Additional Reading:
    - [ ] Mohri, Chapters 2 and 3

## Lesson 9-10: Bayesian learning
  - [x] Maximum likelihood estimation 
  - [x] Bayesian linear regression and curve fitting 
  - [x] Maximum A-Posteriori Estimation 
  - [x] Naive Bayes Classifier 
  - [ ] Additional Reading:
    - [ ] Bishop, Chapter 3, Sections 4.1, 4.2, 4.3

## Lesson 11: Unsupervised learning
  - [x] Dimensionality Reduction: PCA 
  - [x] Clustering: k-Means
  - [x] Density Estimation: Mixture of Gaussians with EM 
  - [ ] Additional Reading:
    - [ ] Bishop, Sections 9.1, 9.2, and 12.1

## Lesson 12-13: Deep learning
  - [x] Perceptrons 
  - [x] Activation functions
  - [x] Multi-layer perceptrons 
  - [x] Backpropagation
  - [x] Stochastic Gradient Descent 
  - [ ] Case Study: Handwritten Digit Recognition
  - [ ] Additional Reading:
    - [ ] Bishop, Section 5.3
    - [ ] Goodfellow, Chapter 6 (see the "Backpropagation" link on the right panel)

## Lesson 13-14: Learning and controlling dynamical systems
  - [x] Recurrent neural networks 
  - [x] Backpropagation through time 
  - [x] Reinforcement learning basic concepts 
  - [x] Q-learning 
  - [x] Bellman equation 
  - [ ] Additional Reading:
    - [ ] Mohri, Chapter 17

## Lesson 15: Kernel methods
  - [x] Feature space transformations
  - [x] Kernel trick 
  - [ ] Mercer's theorem 
  - [ ] Kernel regression 
  - [x] Support vector machines 
  - [ ] Additional Reading:
    - [ ] Bishop, Sections 6.1, 6.2, 7.1

## Lesson 16: Ensemble learning
  - [x] Bootstrap Aggregation (Bagging) 
  - [ ] Boosting (AdaBoost, Gradient Boosting) 
  - [x] Federated learning 
  - [ ] Additional Reading:
    - [ ] Decision Trees: Bishop, Section 14.4
    - [ ] Boosting: Hastie, Chapter 10
    - [ ] Bishop, Section 14.3

